# -*- coding: utf-8 -*-
"""Telecom_Customer_Churn_Prediction_using_PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lCiOmyugP1K-6zBDvm3jBHOSHbLI9vxu

# Project Brief: Telecom Customer Churn Prediction using PySpark

## Background Information

Customer churn is a significant challenge in the telecom industry. Identifying customers who are likely to churn is crucial for implementing proactive measures to retain them. By leveraging PySpark, we can take advantage of its distributed computing capabilities to handle large volumes of data efficiently and build an accurate machine learning model for churn prediction.

## Problem Statement

The goal of this project is to develop a machine learning model using PySpark that accurately
predicts customer churn in a telecom company. The model should achieve a minimum accuracy of
0.8, enabling the company to proactively identify and retain customers at risk of leaving. By
effectively predicting churn, the company can implement targeted retention strategies, reduce
customer attrition, and improve overall business performance.

Guidelines

● **Dataset**: Obtain a telecom customer dataset that includes relevant features such as
customer demographics, usage patterns, service plans, call details, customer complaints,
and churn status. You can use this dataset.

● **Data Preprocessing**: Perform necessary preprocessing steps on the dataset, including handling missing values, feature scaling, encoding categorical variables, and splitting thedata into training and testing sets. Consider using PySpark's DataFrame API for efficient data manipulation.

● **Feature Engineering**: Create new features from the existing dataset that might be helpful for predicting churn. For example, you could calculate metrics such as call duration, average monthly spend, customer tenure, or customer satisfaction scores.

● **Model Selection and Training**: Choose an appropriate machine learning algorithm for churn prediction, considering the nature of the problem and the dataset characteristics. PySpark provides various algorithms such as logistic regression, random forests, gradient boosting, and support vector machines. Experiment with different models and hyperparameter configurations to achieve the desired accuracy of 0.8.

● **Model Evaluation**: Assess the performance of the trained models using appropriate evaluation metrics such as accuracy, precision, recall, and F1-score.

● **Documentation and Reporting**: Maintain clear documentation throughout the project,including details about the dataset, preprocessing steps, feature engineering, model selection, and evaluation results. Summarize the project findings, challenges faced, and lessons learned in a final report.
"""

#install pyspark
!pip install pyspark

## Import the libraries
from pyspark.sql.session import SparkSession
from pyspark import SparkContext
import os
import urllib.request
from pyspark.sql.functions import col
import matplotlib.pyplot as plt
import pyspark.sql.functions as F

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler, MinMaxScaler

# Create a SparkSession
spark = SparkSession.builder.appName("Churn101").getOrCreate()

# Local file path to save the downloaded CSV file
local_file_path = os.getcwd() + "/telecom_dataset.csv"

#Dataset URL
data_url = "https://archive.org/download/telecom_dataset/telecom_dataset.csv"

# Download the CSV file
urllib.request.urlretrieve(data_url, local_file_path)

# Fetch and load dataset: https://archive.org/download/telecom_dataset/telecom_dataset.csv
df = spark.read.csv(local_file_path, header=True)

"""## Data Exploration"""

#Preview df data
df.show()

#Check df shape
# Number of columns
num_columns = len(df.columns)
print("Number of columns:", num_columns)

# Number of rows
num_rows = df.count()
print("Number of rows:", num_rows)

#View df schema
df.printSchema()

"""## Data Preprocessing"""

#Drop nulls
# Drop null rows
df = df.na.drop()

# Verify the result
print("Shape of DataFrame: {} rows, {} columns".format(df.count(), len(df.columns)))

#Drop duplicate row entries
df = df.dropDuplicates()

# Verify the result
print("Shape of DataFrame: {} rows, {} columns".format(df.count(), len(df.columns)))

#Cast numeric columns from string type to integer or float
# Convert Age column to integer type
updated_df = df.withColumn("Age", col("Age").cast("integer"))

# Convert MonthlyCharges column to float type
updated_df = updated_df.withColumn("MonthlyCharges", col("MonthlyCharges").cast("float"))

# Convert TotalCharges column to float type
updated_df = updated_df.withColumn("TotalCharges", col("TotalCharges").cast("float"))

#View updated schema
updated_df.printSchema()

"""## Feature Engineering"""

#Add Age_group column

# Create a new column 'Age_group' based on 'Age' column
df_with_age_group = updated_df.withColumn(
    'Age_group',
    F.when((F.col('Age') >= 0) & (F.col('Age') < 5), '0-4')
    .when((F.col('Age') >= 5) & (F.col('Age') < 10), '5-9')
    .when((F.col('Age') >= 10) & (F.col('Age') < 15), '10-14')
    .when((F.col('Age') >= 15) & (F.col('Age') < 20), '15-19')
    .when((F.col('Age') >= 20) & (F.col('Age') < 25), '20-24')
    .when((F.col('Age') >= 25) & (F.col('Age') < 30), '25-29')
    .when((F.col('Age') >= 30) & (F.col('Age') < 35), '30-34')
    .when((F.col('Age') >= 35) & (F.col('Age') < 40), '35-39')
    .when((F.col('Age') >= 40) & (F.col('Age') < 45), '40-44')
    .when((F.col('Age') >= 45) & (F.col('Age') < 50), '45-49')
    .when((F.col('Age') >= 50) & (F.col('Age') < 55), '50-54')
    .when((F.col('Age') >= 55) & (F.col('Age') < 60), '55-59')
    .when((F.col('Age') >= 60) & (F.col('Age') < 65), '60-64')
    .otherwise('65+')
)

# Show the updated DataFrame
df_with_age_group.show()

# Add a new column 'Month_Total_Ratio' by dividing 'MonthlyCharges' / 'TotalCharges'
df_with_ratio = df_with_age_group.withColumn(
    'Month_Total_Ratio',
    F.col('MonthlyCharges') / F.col('TotalCharges')
)

# Show the updated DataFrame
df_with_ratio.show()

# Feature Scaling
# Define the numerical columns
numerical_columns = ['Age', 'MonthlyCharges', 'TotalCharges', 'Month_Total_Ratio']

# Assemble the numerical columns into a vector column
assembler = VectorAssembler(inputCols=numerical_columns, outputCol="vFeatures")

# Create a MinMaxScaler object
scaler = MinMaxScaler(inputCol="vFeatures", outputCol="scaled_features")

#Categorical encoding

catCols = ['Gender', 'Contract', 'Churn', 'Age_group']
indexers = []

# Iterate over the categorical columns
for col in catCols:
    # Create a StringIndexer object, and specify the input and output columns
    stringindexer = StringIndexer(inputCol=col, outputCol=col+"_index")
    indexers.append(stringindexer)

# Create a Pipeline to chain the StringIndexer stages
#-pipeline = Pipeline(stages=indexers)
pipeline = Pipeline(stages=[assembler, scaler] + indexers)

pipeline_model = pipeline.fit(df_with_ratio)
df_encoded = pipeline_model.transform(df_with_ratio)

"""## Model Selection and Training"""

#Feature and Label Target selection

feature_columns = ["scaled_features", "Gender_index", "Contract_index", "Age_group_index"]
label_column = "Churn_index"

# Vectorize Features
vectorAssembler = VectorAssembler(inputCols=feature_columns, outputCol="mainFeatures")
v_data = vectorAssembler.transform(df_encoded)

#Drop columns which are no longer needed
columns_to_drop = ["CustomerID", "Gender", "Age", "Contract", "MonthlyCharges", "TotalCharges", "Churn", "Age_group", "Month_Total_Ratio", "vFeatures", "scaled_features", "Gender_index", "Contract_index", "Age_group_index"]
v_data = v_data.drop(*columns_to_drop)

# Split the Data for Training and Testing
splits = v_data.randomSplit([0.8, 0.2], seed=42)
train_data = splits[0]
test_data = splits[1]

# Verify the result
#train_data.show(truncate=False)
print("Size of Training DataFrame: {} rows".format(train_data.count()))
print("Size of Testing DataFrame: {} rows".format(test_data.count()))
train_data.printSchema()

# Model Selection and Training
lr = LogisticRegression(labelCol="Churn_index", featuresCol="mainFeatures")

# Create a ParamGrid for grid search
param_grid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="Churn_index")

# Create CrossValidator with the LogisticRegression model and BinaryClassificationEvaluator
cross_validator = CrossValidator(estimator=lr,estimatorParamMaps=param_grid,evaluator=evaluator,numFolds=2)

# Fit the cross-validator to the training data
cvModel = cross_validator.fit(train_data)

# Evaluate the model using the evaluator
accuracy = evaluator.evaluate(cvModel.transform(test_data))

# Get the best model
best_model = cvModel.bestModel

"""## Model Evaluation"""

# Print the best parameters
best_params = best_model.extractParamMap()
print("Best Parameters:")
for param in best_params:
    print(param.name, "=", best_params[param])

# Create a new instance of LogisticRegression
lr_best = LogisticRegression(labelCol="Churn_index", featuresCol="mainFeatures")

# Set the best parameters obtained from the CrossValidator above
lr_best.setParams(aggregationDepth=2,
                  elasticNetParam=0.5,
                  family="auto",
                  fitIntercept=True,
                  maxIter=100,
                  regParam=0.01,
                  standardization=True,
                  threshold=0.5,
                  tol=1e-06)

# Train the logistic regression model with the best parameters
model_best = lr_best.fit(train_data)

# Make predictions on the test data using the best model
predictions_best = model_best.transform(test_data)

# Print model evaluation metrics
evaluator = MulticlassClassificationEvaluator(labelCol="Churn_index")

accuracy_lr = evaluator.evaluate(predictions_best, {evaluator.metricName: "accuracy"})
precision_lr = evaluator.evaluate(predictions_best, {evaluator.metricName: "weightedPrecision"})
recall_lr = evaluator.evaluate(predictions_best, {evaluator.metricName: "weightedRecall"})
f1_lr = evaluator.evaluate(predictions_best, {evaluator.metricName: "f1"})

print("Logistic Regression accuracy:", accuracy_lr)
print("Logistic Regression Precision:", precision_lr)
print("Logistic Regression recall:", recall_lr)
print("Logistic Regression F1-score:", f1_lr)